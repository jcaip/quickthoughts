{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import *\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data \n",
    "newsgroups_train = fetch_20newsgroups(data_home=\"~/workspace/scikit_learn_data\", subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(data_home=\"~/workspace/scikit_learn_data\", subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset 20 NEWS with total lines: 7532\n",
      "{'misc': 0, 'talk': 1, 'sci': 2, 'rec': 3, 'comp': 4, 'alt': 5, 'soc': 6}\n",
      "0 alt.atheism\n",
      "1 comp.graphics\n",
      "2 comp.os.ms-windows.misc\n",
      "3 comp.sys.ibm.pc.hardware\n",
      "4 comp.sys.mac.hardware\n",
      "5 comp.windows.x\n",
      "6 misc.forsale\n",
      "7 rec.autos\n",
      "8 rec.motorcycles\n",
      "9 rec.sport.baseball\n",
      "10 rec.sport.hockey\n",
      "11 sci.crypt\n",
      "12 sci.electronics\n",
      "13 sci.med\n",
      "14 sci.space\n",
      "15 soc.religion.christian\n",
      "16 talk.politics.guns\n",
      "17 talk.politics.mideast\n",
      "18 talk.politics.misc\n",
      "19 talk.religion.misc\n",
      "[3 4 5 1 1 2 6 6 4 4 4 4 1 3 5]\n",
      "[ 7  5  0 17 19 13 15 15  5  1  2  5 17  8  0]\n"
     ]
    }
   ],
   "source": [
    "text, labels = newsgroups_test.data, newsgroups_test.target\n",
    "test_batch_size=1000\n",
    "size = len(labels)\n",
    "print(\"Loaded dataset {} with total lines: {}\".format(\"20 NEWS\", size))\n",
    "\n",
    "top_level_labels = np.copy(labels)\n",
    "top_categories = dict((name, i) for (i, name) in enumerate(set(map(lambda x: x.split('.')[0], newsgroups_test.target_names))))\n",
    "print(top_categories)\n",
    "for i, name in enumerate(newsgroups_test.target_names):\n",
    "    print(i, name)\n",
    "    top = name.split('.')[0]\n",
    "    top_level_labels[labels == i ] = top_categories[top]\n",
    "\n",
    "print(top_level_labels[:15])\n",
    "print(labels[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-16 13:47:03 INFO     collecting all words and their counts\n",
      "2019-07-16 13:47:03 WARNING  Each 'words' should be a list of words (usually unicode strings). First 'words' here is instead plain <class 'str'>.\n",
      "2019-07-16 13:47:03 INFO     PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-07-16 13:47:04 INFO     PROGRESS: at example #10000, processed 12357648 words (10047583/s), 121 word types, 10000 tags\n",
      "2019-07-16 13:47:04 INFO     collected 121 word types and 11314 unique tags from a corpus of 11314 examples and 13781985 words\n",
      "2019-07-16 13:47:04 INFO     Loading a fresh vocabulary\n",
      "2019-07-16 13:47:04 INFO     min_count=1 retains 121 unique words (100% of original 121, drops 0)\n",
      "2019-07-16 13:47:04 INFO     min_count=1 leaves 13781985 word corpus (100% of original 13781985, drops 0)\n",
      "2019-07-16 13:47:04 INFO     deleting the raw counts dictionary of 121 items\n",
      "2019-07-16 13:47:04 INFO     sample=0.001 downsamples 44 most-common words\n",
      "2019-07-16 13:47:04 INFO     downsampling leaves estimated 4083000 word corpus (29.6% of prior 13781985)\n",
      "2019-07-16 13:47:04 INFO     estimated required memory for 121 words and 1000 dimensions: 46284500 bytes\n",
      "2019-07-16 13:47:04 INFO     resetting layer weights\n",
      "2019-07-16 13:47:04 INFO     training model with 8 workers on 121 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-07-16 13:47:05 INFO     EPOCH 1 - PROGRESS: at 18.21% examples, 585908 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:06 INFO     EPOCH 1 - PROGRESS: at 34.76% examples, 573981 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:07 INFO     EPOCH 1 - PROGRESS: at 49.66% examples, 563803 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:08 INFO     EPOCH 1 - PROGRESS: at 65.87% examples, 561612 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:09 INFO     EPOCH 1 - PROGRESS: at 83.31% examples, 563601 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-16 13:47:10 INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-16 13:47:10 INFO     EPOCH - 1 : training on 13781985 raw words (3354784 effective words) took 5.9s, 567015 effective words/s\n",
      "2019-07-16 13:47:11 INFO     EPOCH 2 - PROGRESS: at 18.63% examples, 607390 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:12 INFO     EPOCH 2 - PROGRESS: at 36.19% examples, 601215 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:13 INFO     EPOCH 2 - PROGRESS: at 53.43% examples, 607204 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:14 INFO     EPOCH 2 - PROGRESS: at 69.96% examples, 596059 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:15 INFO     EPOCH 2 - PROGRESS: at 86.71% examples, 585717 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-16 13:47:16 INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-16 13:47:16 INFO     EPOCH - 2 : training on 13781985 raw words (3356753 effective words) took 5.8s, 583217 effective words/s\n",
      "2019-07-16 13:47:17 INFO     EPOCH 3 - PROGRESS: at 17.70% examples, 579285 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:18 INFO     EPOCH 3 - PROGRESS: at 34.03% examples, 564155 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:19 INFO     EPOCH 3 - PROGRESS: at 50.51% examples, 573042 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:20 INFO     EPOCH 3 - PROGRESS: at 66.97% examples, 572067 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:21 INFO     EPOCH 3 - PROGRESS: at 84.73% examples, 572803 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-16 13:47:22 INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-16 13:47:22 INFO     EPOCH - 3 : training on 13781985 raw words (3356758 effective words) took 5.8s, 575804 effective words/s\n",
      "2019-07-16 13:47:23 INFO     EPOCH 4 - PROGRESS: at 19.67% examples, 636178 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:24 INFO     EPOCH 4 - PROGRESS: at 37.57% examples, 624856 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:25 INFO     EPOCH 4 - PROGRESS: at 54.81% examples, 619777 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:26 INFO     EPOCH 4 - PROGRESS: at 71.53% examples, 610228 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:27 INFO     EPOCH 4 - PROGRESS: at 89.93% examples, 604072 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-16 13:47:27 INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-16 13:47:27 INFO     EPOCH - 4 : training on 13781985 raw words (3354983 effective words) took 5.6s, 600722 effective words/s\n",
      "2019-07-16 13:47:28 INFO     EPOCH 5 - PROGRESS: at 16.58% examples, 532300 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:29 INFO     EPOCH 5 - PROGRESS: at 33.71% examples, 556254 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:30 INFO     EPOCH 5 - PROGRESS: at 50.26% examples, 568954 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:31 INFO     EPOCH 5 - PROGRESS: at 66.75% examples, 567970 words/s, in_qsize 16, out_qsize 0\n",
      "2019-07-16 13:47:32 INFO     EPOCH 5 - PROGRESS: at 84.30% examples, 569823 words/s, in_qsize 15, out_qsize 0\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-16 13:47:33 INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-16 13:47:33 INFO     EPOCH - 5 : training on 13781985 raw words (3356100 effective words) took 5.9s, 569536 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-16 13:47:33 INFO     training on a 68909925 raw words (16779378 effective words) took 29.0s, 578219 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#train d2v model\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(newsgroups_train.data)]\n",
    "d2v = Doc2Vec(documents, vector_size=1000, window=2, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-16 13:48:36 INFO     loading projection weights from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-16 13:50:42 INFO     loaded (400000, 300) matrix from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored successfully from /home/jcjessecai/workspace/taboola/quickthoughts/checkpoints\n"
     ]
    }
   ],
   "source": [
    "#load qt model\n",
    "checkpoint_dir = '/home/jcjessecai/workspace/taboola/quickthoughts/checkpoints'\n",
    "with open(\"{}/config.json\".format(checkpoint_dir)) as fp:\n",
    "    CONFIG = json.load(fp)\n",
    "\n",
    "WV_MODEL = api.load(CONFIG['embedding'])\n",
    "qt = QuickThoughts(WV_MODEL, hidden_size=CONFIG['hidden_size'])\n",
    "trained_params = torch.load(\"{}/checkpoint_latest.pth\".format(checkpoint_dir))\n",
    "qt.load_state_dict(trained_params['state_dict'])\n",
    "qt = qt.cuda()\n",
    "qt.eval()\n",
    "print(\"Restored successfully from {}\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing     8 batches of size  1000\n",
      "Test feature matrix of shape: (7532, 1000)\n"
     ]
    }
   ],
   "source": [
    "#encode data\n",
    "def make_batch(j):\n",
    "    \"\"\"Processes one test batch of the test datset\"\"\"\n",
    "    stop_idx = min(size, j+test_batch_size)\n",
    "    batch_text, batch_labels  = text[j:stop_idx], labels[j:stop_idx]\n",
    "    data = list(map(lambda x: torch.LongTensor(prepare_sequence(x, WV_MODEL.vocab, no_zeros=True)), batch_text))\n",
    "    for i in data:\n",
    "        if len(i) == 0:\n",
    "            print(i)\n",
    "            input()\n",
    "    packed = safe_pack_sequence(data).cuda()\n",
    "    return qt(packed).cpu().detach().numpy()\n",
    "\n",
    "feature_list = [make_batch(i) for i in range(0, size, test_batch_size)]\n",
    "print(\"Processing {:5d} batches of size {:5d}\".format(len(feature_list), test_batch_size))\n",
    "qt_features = np.concatenate(feature_list)\n",
    "print(\"Test feature matrix of shape: {}\".format(qt_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 1000)\n"
     ]
    }
   ],
   "source": [
    "d2v_features = np.vstack([d2v.infer_vector(doc) for doc in newsgroups_test.data])\n",
    "print(d2v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit logistic model with s:   1 and acc: 46.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit logistic model with s:   1 and acc: 64.90%\n"
     ]
    }
   ],
   "source": [
    "#first we compare embedding performance by fitting binary classifier on top\n",
    "s=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(d2v_features, top_level_labels)\n",
    "clf = LogisticRegression(solver='sag', C=s)\n",
    "clf.fit(X_train, y_train)\n",
    "acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Fit logistic model with s: {:3d} and acc: {:.2%}\".format(s, acc))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(qt_features, top_level_labels)\n",
    "clf = LogisticRegression(solver='sag', C=s)\n",
    "clf.fit(X_train, y_train)\n",
    "acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Fit logistic model with s: {:3d} and acc: {:.2%}\".format(s, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_predicted  = KMeans(n_clusters=7, n_jobs=20).fit_predict(qt_features)\n",
    "d2v_predicted  = KMeans(n_clusters=7, n_jobs=20).fit_predict(d2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10898983664881542\n",
      "0.045095497905550064\n",
      "0.15014304339815002\n",
      "0.0669735747454644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(adjusted_rand_score(top_level_labels, qt_predicted))\n",
    "print(adjusted_rand_score(top_level_labels, d2v_predicted))\n",
    "\n",
    "print(adjusted_mutual_info_score(top_level_labels, qt_predicted))\n",
    "print(adjusted_mutual_info_score(top_level_labels, d2v_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_embedded = TSNE(n_components=2, verbose=1).fit_transform(features[:5000])\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[labels[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[predicted_spectral[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[predicted_kmeans[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
